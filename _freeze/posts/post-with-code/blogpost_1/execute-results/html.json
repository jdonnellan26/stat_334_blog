{
  "hash": "6811d769c960856c0f8be00ac1883aa8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Blog Post 1\"\nauthor: \"John Donnellan\"\ndate: \"2026-02-04\"\ncategories: [news, code, analysis]\n---\n\n**Introduction:**\n\nI am working with the Pixar film data set posted from TidyTuesday 2025-03-11. The data comes from the {pixarfilms} R package by Eric Luang. There are 23 observations in this data set. I am most interested in observing the differences in ratings between the main stream critics by Rotten Tomatoes, Meta Critics, and Critics Choice.\n\n**The variables I will be examining:**\n\n-   ratings: Name of rating source (Rotten Tomatoes, Meta Critics, and Critics Choice)\n\n-   avg_rating: The average rating given on a particular movie by each of the three rating sources\n\n-   value: The score out of a hundred given by Rotten Tomatoes, Meta Critics, and Critics Choice\n\n-   period: Whether the movie was released before or after 2010\n\n-   film: The title of the film\n\n**Loading the Data:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(pixarfilms)\npixar_films <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/pixar_films.csv')\npublic_response <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/public_response.csv')\n\n\npixar <-\n  public_response %>%\n  left_join(box_office, by = \"film\") %>%\n  left_join(pixar_films, by = \"film\") %>%\n  group_by(film) %>%\n  pivot_longer(cols = c(\"rotten_tomatoes\", \"metacritic\", \"critics_choice\"),\n               names_to = \"ratings\",\n               values_to = \"value\") %>%\n  mutate(ratings = case_when(\n    ratings == \"metacritic\" ~ \"Metacritic\",\n    ratings == \"rotten_tomatoes\" ~ \"Rotten Tomatoes\",\n    ratings == \"critics_choice\" ~ \"Critics Choice\"\n  )) %>%\n  select(-cinema_score, - budget ) %>%\n  drop_na()%>%\n  mutate(avg_rating = mean(value)) %>%\n  mutate( period = if_else(release_date <= as.Date(\"2009-12-31\"),\"Before 2010\",\"2010 and After\"), \n          # I got some help from the web to transform the release date to organize films by year,\n    period = fct_relevel(period, c(\"Before 2010\", \"2010 and After\")))%>%\n  arrange(desc(avg_rating))\n```\n:::\n\n\n**Graph #1**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  ggplot(data = pixar, aes(x =fct_reorder(film, avg_rating), y = value, col = ratings)) +\n  geom_point() +\n  geom_line(aes(group = ratings)) +\n  scale_color_viridis_d() +\n  labs(x = \"Pixar film\", y = \"Rating value\", title = \"Ratings by Film\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) \n```\n\n::: {.cell-output-display}\n![](blogpost_1_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n**Interpretation:**\n\nThis graph shows Pixar films ordered from lowest to highest average rating, with scores from Rotten Tomatoes, Metacritic, and Critics Choice plotted for each movie. While Rotten Tomatoes generally gives higher scores and Metacritic gives lower ones, all three sources move together across films. This indicates strong agreement among critics about which Pixar movies are better or worse, even if they use slightly different scoring standards.\n\n**Graph #2**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = pixar, aes(y = value, x = ratings, color = ratings))+\n  geom_boxplot()+\n  scale_color_viridis_d() +\n  labs(x = \"Ratings\", y = \"Value\", color = \"Ratings\", title = \"Distribution of Ratings by Source\")+\n   theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](blogpost_1_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n**Interpretation:**\n\nThis boxplot compares the overall distribution of ratings from Critics Choice, Metacritic, and Rotten Tomatoes. Rotten Tomatoes has the highest median rating, suggesting it is the most generous source, while Metacritic has the lowest median and the widest spread, indicating stricter and more variable scoring. Critics Choice falls between the two in both typical score and variability. The presence of lower outlier Cars 2 with an average rating of approximately 55.\n\n**Graph #3**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pixar, aes(x = period, y = value, fill = period)) +\n  geom_boxplot() +\n  facet_wrap(~ ratings) +\n  labs(x = \"Release Period\", y = \"Value\", title = \"Before vs After 2010 by Rating Source\") +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](blogpost_1_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n**Interpretation:**\n\nThis graph compares ratings before and after 2010 separately for each rating source. For all three sources, the median rating is higher for films released before 2010, suggesting Pixarâ€™s earlier movies were more consistently well received. The post-2010 period shows greater spread and lower minimum values, especially for Metacritic and Rotten Tomatoes. Overall, this indicates that more recent Pixar films have had more mixed and less consistently strong critical reception.\n\n**Stat 334 Connection:**\n\nThese visualizations are effective because they use they rely on different graphs using position, grouping, and faceting to make differences across rating sources and time periods easy to see. The visuals eliminate clutter, and focus attention as appropriate to answer my question of interest: What are the differences in ratings between the main stream critics by Rotten Tomatoes, Meta Critics, and Critics Choice.\n\n**Conclusion:**\n\nAcross these visualizations, all three rating sources show a similar pattern from movie to movie. However, Rotten Tomatoes consistently rates higher scores, while Metacritic tends to rate the same films more conservatively. This suggests that although the sources differ in their scoring standards, they largely agree on which movies are better or worse.\n\nOne limitation of this analysis is that it is based on only 23 Pixar films, which means individual movies and outliers can have a large influence on the patterns we observe. The data also only three critic sources even though they each use different scoring standards. In addition, the graphs only show the differences but cannot describe why the differences occur. If I could do this again I would find a data set of all animated movies to see how the different rating sources grade for different companies.\n",
    "supporting": [
      "blogpost_1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}